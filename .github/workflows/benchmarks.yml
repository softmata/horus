name: Benchmark CI

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]
  schedule:
    # Run weekly on Monday at 00:00 UTC
    - cron: '0 0 * * 1'
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  # Regression threshold: fail if latency increases by more than this percentage
  REGRESSION_THRESHOLD: 10

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for baseline comparison

    - name: Setup Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          linux-tools-common \
          linux-tools-generic \
          gnuplot \
          libudev-dev

    - name: Configure CPU for benchmarks
      run: |
        # Disable CPU frequency scaling for consistent results (may fail on some runners)
        echo "performance" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true

        # Disable turbo boost
        echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo || true

    - name: Cache cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

    - name: Download baseline (if exists)
      uses: actions/cache@v4
      with:
        path: benchmarks/baseline.json
        key: benchmark-baseline-${{ github.base_ref || 'main' }}
        restore-keys: |
          benchmark-baseline-main
          benchmark-baseline-

    - name: Build benchmarks
      run: |
        cd benchmarks
        cargo build --release --benches

    - name: Run Link performance benchmarks
      run: |
        cd benchmarks
        cargo bench --bench link_performance -- --noplot

    - name: Run production message benchmarks
      run: |
        cd benchmarks
        cargo bench --bench production_messages -- --noplot

    - name: Generate benchmark report
      run: |
        cd benchmarks
        # Criterion generates HTML reports in target/criterion
        mkdir -p benchmark-report
        cp -r target/criterion/* ./benchmark-report/ || true

    - name: Check for performance regression
      id: regression-check
      run: |
        cd benchmarks
        # Run regression check with threshold
        python3 scripts/check_regression.py \
          target/criterion \
          --threshold ${{ env.REGRESSION_THRESHOLD }} \
          --baseline-file baseline.json \
          --output-markdown /tmp/benchmark-report.md

    - name: Save new baseline (on main branch)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        cd benchmarks
        python3 scripts/check_regression.py \
          target/criterion \
          --save-baseline \
          --baseline-file baseline.json

    - name: Update baseline cache (on main)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      uses: actions/cache/save@v4
      with:
        path: benchmarks/baseline.json
        key: benchmark-baseline-main

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmarks/benchmark-report
          benchmarks/target/criterion
          benchmarks/baseline.json

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          let summary = "";
          try {
            summary = fs.readFileSync('/tmp/benchmark-report.md', 'utf8');
          } catch (e) {
            summary = "## Benchmark Results\n\n:warning: Could not parse benchmark results.";
          }

          // Add footer
          summary += "\n\n---\n";
          summary += `*Regression threshold: ${{ env.REGRESSION_THRESHOLD }}%*\n`;
          summary += `*Commit: ${{ github.sha }}*`;

          // Find and update existing comment or create new one
          const { data: comments } = await github.rest.issues.listComments({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
          });

          const botComment = comments.find(comment =>
            comment.user.type === 'Bot' &&
            comment.body.includes('Benchmark Results')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              comment_id: botComment.id,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          } else {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }

  benchmark-compare:
    name: Compare with ROS2
    runs-on: ubuntu-22.04
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - uses: actions/checkout@v4

    - name: Setup ROS2
      uses: ros-tooling/setup-ros@v0.7
      with:
        required-ros-distributions: humble

    - name: Install ROS2 performance tools
      run: |
        sudo apt-get update
        sudo apt-get install -y ros-humble-performance-test || true

    - name: Run comparison benchmarks
      run: |
        cd benchmarks
        cargo run --release --bin comparison_runner || echo "Comparison runner not available"

    - name: Generate comparison report
      run: |
        cd benchmarks
        if [ -f scripts/generate_comparison.py ]; then
          python3 scripts/generate_comparison.py > comparison.md
        else
          echo "# Comparison Report" > comparison.md
          echo "Comparison script not available" >> comparison.md
        fi

    - name: Upload comparison
      uses: actions/upload-artifact@v4
      with:
        name: ros2-comparison
        path: benchmarks/comparison.md

  publish-results:
    name: Publish to GitHub Pages
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: github.ref == 'refs/heads/main'

    permissions:
      contents: read
      pages: write
      id-token: write

    steps:
    - uses: actions/checkout@v4

    - name: Download artifacts
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results
        path: ./public

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload to Pages
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./public

    - name: Deploy to GitHub Pages
      uses: actions/deploy-pages@v4
